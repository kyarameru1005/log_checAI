#!/home/kyarameru/log_checAI/.venv/bin/python
# --- å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ---
import time
import joblib
import subprocess
import json
from datetime import datetime, timezone
from zoneinfo import ZoneInfo
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import apache_log_parser
from pprint import pprint

# --- è¨­å®š ---
WATCH_DIR = "/var/log/apache2"  # ç›£è¦–å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆApacheãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
LOG_FORMAT = "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\""  # Apacheãƒ­ã‚°ã®æ›¸å¼
parser = apache_log_parser.make_parser(LOG_FORMAT)  # ãƒ­ã‚°ãƒ‘ãƒ¼ã‚µã®ä½œæˆ
ANALYSIS_FILE = "analysis_results.jsonl"  # åˆ†æçµæœã®ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
IP_COUNTS_FILE = "ip_access_counts.json"  # IPã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«
ANOMALOUS_PATH_COUNTS_FILE = "anomalous_path_counts.json"  # æ”»æ’ƒãƒ‘ã‚¹ã®å›æ•°ã‚’è¨˜éŒ²ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«

# --- æ—¥æ™‚ã‚’JSONã§æ‰±ã†ãŸã‚ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ ---
class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime): return obj.isoformat()
        return super().default(obj)

# --- å¤–éƒ¨ãƒªã‚¹ãƒˆï¼ˆãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãƒ»ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼‰ã®èª­ã¿è¾¼ã¿é–¢æ•° ---
def load_list_from_file(filename):
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return [line.strip() for line in f if line.strip() and not line.startswith('#')]
    except FileNotFoundError:
        return []

WHITELIST_PATTERNS = load_list_from_file('whitelist.txt')  # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª­ã¿è¾¼ã¿
BLACKLIST_PATTERNS = load_list_from_file('blacklist.txt')  # ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®èª­ã¿è¾¼ã¿

# --- AIãƒ¢ãƒ‡ãƒ«ã¨ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ã®èª­ã¿è¾¼ã¿ ---
try:
    model = joblib.load('log_anomaly_model.joblib')  # ç•°å¸¸æ¤œçŸ¥ãƒ¢ãƒ‡ãƒ«
    vectorizer = joblib.load('tfidf_vectorizer.joblib')  # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶
except FileNotFoundError:
    print("[ã‚¨ãƒ©ãƒ¼] AIãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚train_model.pyã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    exit()

# --- ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆåˆ¤å®šé–¢æ•° ---
def is_whitelisted(request_line):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆã«è©²å½“ã™ã‚‹ã‹åˆ¤å®š"""
    for pattern in WHITELIST_PATTERNS:
        if pattern.lower() in request_line.lower(): return True
    return False

# --- ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆåˆ¤å®šé–¢æ•° ---
def is_blacklisted(request_line):
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆã«è©²å½“ã™ã‚‹ã‹åˆ¤å®š"""
    for pattern in BLACKLIST_PATTERNS:
        if pattern.lower() in request_line.lower(): return True
    return False

# --- AIã«ã‚ˆã‚‹ç•°å¸¸åˆ¤å®šé–¢æ•° ---
def predict_log_anomaly(log_text):
    """AIãƒ¢ãƒ‡ãƒ«ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ç•°å¸¸åˆ¤å®šã‚’è¡Œã†"""
    vectorized_text = vectorizer.transform([log_text])
    prediction = model.predict(vectorized_text)[0]
    return bool(prediction)

# --- æ”»æ’ƒæ¤œçŸ¥æ™‚ã®åˆ†æã‚·ãƒ¼ã‚±ãƒ³ã‚¹ ---
def trigger_analysis_sequence(log_data, detection_method):
    """æ”»æ’ƒæ¤œçŸ¥æ™‚ã«ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã§å†ç¾ãƒ»åˆ†æã—ã€çµæœã‚’è¨˜éŒ²ã™ã‚‹"""
    print(f"--- ğŸš€ åˆ†æã‚·ãƒ¼ã‚±ãƒ³ã‚¹é–‹å§‹ (æ¤œçŸ¥æ–¹æ³•: {detection_method}) ---")
    container_id = None
    try:
        print("1. Apacheã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒã‚’èµ·å‹•ä¸­...")
        command = ["docker", "run", "-d", "--rm", "twinai-apache-sandbox"]
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        container_id = result.stdout.strip()
        print(f"   âœ… èµ·å‹•æˆåŠŸ (ã‚³ãƒ³ãƒ†ãƒŠID: {container_id[:12]})")
    except Exception as e:
        print(f"[ã‚¨ãƒ©ãƒ¼] ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã®èµ·å‹•ã«å¤±æ•—: {e}")
        return

    reproduce_output, filesystem_changes = "", ""
    try:
        print(f"\n2. ã‚³ãƒ³ãƒ†ãƒŠã«å¯¾ã—ã¦æ”»æ’ƒã‚’å†ç¾ä¸­...")
        request_path = log_data.get('request_first_line', '').split()[1]
        reproduce_command = ["docker", "exec", container_id, "curl", f"http://localhost:80{request_path}"]
        reproduce_result = subprocess.run(reproduce_command, capture_output=True, text=True, check=False)
        reproduce_output = reproduce_result.stdout.strip() if reproduce_result.stdout else reproduce_result.stderr.strip()
        print("   âœ… å†ç¾å®Œäº†ã€‚")
    except Exception as e:
        reproduce_output = f"å†ç¾ã‚¨ãƒ©ãƒ¼: {e}"
    try:
        print("\n3. ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å¤‰åŒ–ã‚’è¦³å¯Ÿä¸­...")
        diff_command = ["docker", "diff", container_id]
        diff_result = subprocess.run(diff_command, capture_output=True, text=True, check=True)
        filesystem_changes = diff_result.stdout.strip()
        print("   âœ… è¦³å¯Ÿå®Œäº†ã€‚")
    except Exception as e:
        filesystem_changes = f"å·®åˆ†æ¤œçŸ¥ã‚¨ãƒ©ãƒ¼: {e}"
    try:
        print(f"\n4. åˆ†æçµæœã‚’ {ANALYSIS_FILE} ã«è¨˜éŒ²ä¸­...")
        analysis_record = {
            "analysis_timestamp": datetime.now(ZoneInfo("Asia/Tokyo")).isoformat(),
            "detection_method": detection_method,
            "original_log": log_data,
            "reproduction_result": reproduce_output,
            "filesystem_changes": filesystem_changes.split('\n') if filesystem_changes else []
        }
        with open(ANALYSIS_FILE, "a") as f:
            f.write(json.dumps(analysis_record, cls=DateTimeEncoder) + "\n")
        print("   âœ… è¨˜éŒ²å®Œäº†ã€‚")
    except Exception as e:
        print(f"[ã‚¨ãƒ©ãƒ¼] çµæœã®è¨˜éŒ²ã«å¤±æ•—: {e}")
    finally:
        if container_id:
            print("\n5. ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒã‚’ç ´æ£„ã—ã¾ã™ã€‚")
            subprocess.run(["docker", "stop", container_id], capture_output=True, text=True)

# --- ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ© ---
class ChangeHandler(FileSystemEventHandler):
    def __init__(self, state, ip_counts, path_counts):
        self.last_positions = {}  # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚èª­ã¿å–ã‚Šä½ç½®ã‚’è¨˜éŒ²
        self.state = state  # å…±æœ‰çŠ¶æ…‹ï¼ˆæœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ™‚åˆ»ãªã©ï¼‰
        self.ip_counts = ip_counts  # IPã”ã¨ã®ã‚¢ã‚¯ã‚»ã‚¹å›æ•°è¾æ›¸
        self.path_counts = path_counts # æ”»æ’ƒãƒ‘ã‚¹ã‚«ã‚¦ãƒ³ãƒˆç”¨ã®è¾æ›¸

    def on_modified(self, event):
        # access.logãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã®ã¿å‡¦ç†
        if event.is_directory or 'access.log' not in event.src_path: return
        try:
            with open(event.src_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_positions.get(event.src_path, 0))
                new_lines = f.readlines()
                self.last_positions[event.src_path] = f.tell()
        except Exception: return

        for line in new_lines:
            if not line.strip(): continue
            try:
                log_data = parser(line)  # ãƒ­ã‚°1è¡Œã‚’ãƒ‘ãƒ¼ã‚¹
                ip_address = log_data.get('remote_host')
                if ip_address:
                    self.ip_counts[ip_address] = self.ip_counts.get(ip_address, 0) + 1  # IPã”ã¨ã«ã‚«ã‚¦ãƒ³ãƒˆ
                
                request_line = log_data.get('request_first_line', '')
                if is_whitelisted(request_line):
                    self.state['last_message_time'] = datetime.now()
                    continue

                is_detected, detection_method = False, ""
                if is_blacklisted(request_line):
                    is_detected, detection_method = True, "ãƒ–ãƒ©ãƒƒã‚¯ãƒªã‚¹ãƒˆ"
                elif predict_log_anomaly(request_line):
                    is_detected, detection_method = True, "AI"

                if is_detected:
                    utc_time = log_data.get('time_received_datetimeobj')
                    log_time_str = utc_time.replace(tzinfo=timezone.utc).astimezone(ZoneInfo("Asia/Tokyo")).strftime('%Y-%m-%d %H:%M:%S') if utc_time else "æ™‚åˆ»ä¸æ˜"
                    
                    print(f"\nğŸš¨ğŸš¨ğŸš¨ã€{detection_method}ã§ç•°å¸¸ã‚’æ¤œçŸ¥ã€‘ğŸš¨ğŸš¨ğŸš¨")
                    print(f"ç™ºç”Ÿæ™‚åˆ» (JST): {log_time_str}")
                    pprint(log_data)
                    
                    self.state['last_message_time'] = datetime.now()

                    # --- æ”»æ’ƒã•ã‚ŒãŸãƒ‘ã‚¹ã‚’ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ— ---
                    request_path = log_data.get('request_url_path')
                    if request_path:
                        self.path_counts[request_path] = self.path_counts.get(request_path, 0) + 1

                    trigger_analysis_sequence(log_data, detection_method)
            except Exception as e:
                print(f"[è­¦å‘Š] ãƒ­ã‚°1è¡Œã®å‡¦ç†ã«å¤±æ•—: {e}")

if __name__ == "__main__":
    print("\n--- TwinAI - Log Sentinel (v2.5 æ”»æ’ƒãƒ‘ã‚¹é›†è¨ˆç‰ˆ) èµ·å‹• ---")

    # --- èµ·å‹•æ™‚ã«IPã¨ãƒ‘ã‚¹ã®ã‚«ã‚¦ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ ---
    ip_counts_data = {}
    path_counts_data = {}
    try:
        with open(IP_COUNTS_FILE, 'r', encoding='utf-8') as f: ip_counts_data = json.load(f)
        print("--- éå»ã®IPã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ---")
    except (FileNotFoundError, json.JSONDecodeError):
        print("--- IPã‚¢ã‚¯ã‚»ã‚¹å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ–°è¦ã«ä½œæˆã—ã¾ã™ ---")
    try:
        with open(ANOMALOUS_PATH_COUNTS_FILE, 'r', encoding='utf-8') as f: path_counts_data = json.load(f)
        print("--- éå»ã®æ”»æ’ƒãƒ‘ã‚¹å±¥æ­´ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ ---")
    except (FileNotFoundError, json.JSONDecodeError):
        print("--- æ”»æ’ƒãƒ‘ã‚¹å±¥æ­´ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€æ–°è¦ã«ä½œæˆã—ã¾ã™ ---")

    shared_state = {"last_message_time": datetime.now()}
    event_handler = ChangeHandler(shared_state, ip_counts_data, path_counts_data)
    observer = Observer()
    observer.schedule(event_handler, WATCH_DIR, recursive=True)
    observer.start()
    
    print("--- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ­ã‚°ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™ (Ctrl+Cã§çµ‚äº†) ---")

    try:
        while True:
            time.sleep(1)
            elapsed = (datetime.now() - shared_state["last_message_time"]).total_seconds()
            if elapsed > 60:
                try:
                    jst_now = datetime.now(ZoneInfo("Asia/Tokyo")).strftime('%H:%M:%S')
                    print(f"âœ… [ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸] {jst_now}ç¾åœ¨ã€æ–°ãŸãªç•°å¸¸ã¯æ¤œçŸ¥ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    shared_state["last_message_time"] = datetime.now()
                    with open(IP_COUNTS_FILE, 'w', encoding='utf-8') as f: json.dump(ip_counts_data, f, indent=4)
                    with open(ANOMALOUS_PATH_COUNTS_FILE, 'w', encoding='utf-8') as f: json.dump(path_counts_data, f, indent=4)
                except Exception as e:
                    print(f"[DEBUG][ä¾‹å¤–] 1åˆ†ã”ã¨å‡ºåŠ›å‡¦ç†ã§ä¾‹å¤–: {e}")

    except KeyboardInterrupt:
        print("\n--- ç›£è¦–ã‚’çµ‚äº†ã—ã¾ã™ã€‚æœ€çµ‚çµæœã‚’ä¿å­˜ä¸­... ---")
    except Exception as e:
        print(f"[DEBUG][ä¾‹å¤–] ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã§ä¾‹å¤–: {e}")
    finally:
        # --- çµ‚äº†æ™‚ã«æœ€çµ‚çš„ãªIPã¨ãƒ‘ã‚¹ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’ä¿å­˜ ---
        with open(IP_COUNTS_FILE, 'w', encoding='utf-8') as f:
            sorted_ips = dict(sorted(ip_counts_data.items(), key=lambda item: item[1], reverse=True))
            json.dump(sorted_ips, f, indent=4)
        with open(ANOMALOUS_PATH_COUNTS_FILE, 'w', encoding='utf-8') as f:
            sorted_paths = dict(sorted(path_counts_data.items(), key=lambda item: item[1], reverse=True))
            json.dump(sorted_paths, f, indent=4)
        print("--- IPã‚¢ã‚¯ã‚»ã‚¹å›æ•°ã¨æ”»æ’ƒãƒ‘ã‚¹å›æ•°ã®ä¿å­˜ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ ---")
        observer.stop()
        
    observer.join()